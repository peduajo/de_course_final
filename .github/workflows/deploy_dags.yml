name: Deploy Airflow DAGs to Cloud Composer

on:
  # Desplegamos cada vez que se hace push a dev o main
  push:
    branches:
      - dev        # → entorno DEV
      - main       # → entorno PROD

jobs:
  deploy-dags:
    runs-on: ubuntu-latest

    # GitHub nos dará un ID-token para el paso de autenticación OIDC
    permissions:
      id-token: write       # imprescindible para Workload Identity Federation
      contents: read        # necesario para hacer checkout del repo

    # Variables comunes al job
    env:
      REGION: europe-west1          # región donde creaste los Composer
      COMPOSER_DEV: composer-dev    # nombre exacto de tu entorno DEV
      COMPOSER_PROD: composer-prod  # nombre exacto de tu entorno PROD
      DAG_FOLDER: ./dags         # carpeta de DAGs dentro del repo
      UTILS_FOLDER: ./utils
      REQS_FILE: ./requirements.txt
      PROJECT_ID: taxy-rides-ny-459209
      GCP_PATH_SPARK_JOB_SCRIPT: jobs/transform.py

    steps:
    # 1) Clonamos el commit que acaba de llegar
    - name: Checkout repository
      uses: actions/checkout@v4

    # 2) Autenticarnos en Google Cloud *sin* JSON (OIDC + Workload Identity)
    - id: auth
      name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        workload_identity_provider: ${{ secrets.GCP_WIF_PROVIDER }}   # p.e. 'projects/123.../providers/github-provider'
        service_account:           ${{ secrets.GCP_DEPLOY_SA }}       # 'github-composer-deployer@taxy-rides-ny-459209.iam.gserviceaccount.com'

    # 3) Instalar gcloud en el runner ya autenticado
    - name: Setup gcloud
      uses: google-github-actions/setup-gcloud@v2

    # 4) Copiar los DAGs al bucket del entorno adecuado
    - name: Upload DAGs to Cloud Composer
      run: |
        # main → PROD, resto → DEV
        if [[ "${GITHUB_REF##*/}" == "main" ]]; then
          ENV_NAME="${COMPOSER_PROD}"
          SUFFIX="_prod"
        else
          ENV_NAME="${COMPOSER_DEV}"
          SUFFIX="_dev"
        fi

        BUCKET="airplanes-bucket${SUFFIX}"
        TMP_BUCKET="dataproc-temp-europe-west1-828225226997-fckhkym8${SUFFIX}"
        BQ_DS="flights_dataset${SUFFIX}"

        echo "Deploying DAGs to ${ENV_NAME} in ${REGION}…"

        gcloud composer environments storage dags import \
          --environment="${ENV_NAME}" \
          --location="${REGION}" \
          --source="${DAG_FOLDER}/*" \
          --quiet

        echo "Updating PyPI packages in ${ENV_NAME} …"
        set +e   # desactiva exit-on-error temporalmente
        OUTPUT=$(gcloud composer environments update "${ENV_NAME}" \
                  --location "$REGION" \
                  --update-pypi-packages-from-file requirements.txt 2>&1)
        STATUS=$?
        set -e   # vuelve a activar

        if [[ $STATUS -eq 1 && "$OUTPUT" == *"does not result in any change"* ]]; then
          echo "requirements.txt is identical — nothing to update."
        elif [[ $STATUS -ne 0 ]]; then
          echo "$OUTPUT"
          exit $STATUS         # error real: aborta el job
        else
          echo "$OUTPUT"       # paquetes instalados con éxito
        fi

        # CI/CD: parte extra del workflow después de instalar requirements
        gcloud composer environments update ${ENV_NAME} \
        --location "${REGION}" \
        --update-env-variables=TF_VAR_bucket_name=${BUCKET},TF_VAR_tmp_bucket_name=${TMP_BUCKET},TF_VAR_bq_dataset_name=${BQ_DS},TF_VAR_project_id=${PROJECT_ID},REGION=${REGION},GCP_PATH_SPARK_JOB_SCRIPT=${GCP_PATH_SPARK_JOB_SCRIPT}
